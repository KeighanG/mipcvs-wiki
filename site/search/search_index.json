{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome to Documentation Wiki on CVs and how to use them</p> <p>Documentation in progress</p> <p>The contents of the pages are currently in development, and many aspects still in flux. </p>"},{"location":"#projects","title":"Projects","text":"<p>Currently there are several projects available. These are:</p> <ul> <li><code>CMIP6Plus</code> - the current iteration of CMIP6 related sumbissions allowing us to test and prototype cutting edge improvements to usability and infrastructure and assess their suitability going forward. </li> <li><code>INPUT4MIPs</code> - forcings</li> <li><code>OBS4MIPs</code> - observations</li> </ul> <p>The structure of this documentation will be allow the subtitution of your relevant mip in the <code>&lt;mip_name&gt;</code> tags. Should your MIP not be features, please speak to us regarding resources and requirements. </p>"},{"location":"#useful-links","title":"Useful Links","text":"<p>There are number of links that may infulence your workflow. Many of these can be found on the  CMIP-IPO website .</p>"},{"location":"#iframe-test","title":"iframe test","text":""},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"99_Acknowlegements/","title":"Acknowledgement","text":"<p>The work within most repositories can be attributed to a large number of contributers. A collection of those involved can be found here. </p>"},{"location":"99_Acknowlegements/#mip-cmor-tables","title":"MIP CMOR Tables","text":"<p>The repository content has been developed by climate and computer scientists representing the Coupled Model Intercomparison Project phase 6 (CMIP6) and earlier phases, including those from climate modeling groups and model intercomparison projects (MIPs) worldwide. A special mention to Dr. Martin Juckes from the UK Centre for Environmental Data Analysis (CEDA) for leading efforts in the CMIP6 Data Request. The structure of repository content and tools required to maintain it was developed by climate and computer scientists from the Program for Climate Model Diagnosis and Intercomparison (PCMDI) at Lawrence Livermore National Laboratory (LLNL) and the UK MetOffice, with assistance from colleagues at the Coupled Model Intercomparison Project International Project Office (CMIP-IPO), the Deutsches Klimarechenzentrum (DKRZ) in Germany and the members of the Infrastructure for the European Network for Earth System Modelling (IS-ENES) consortium.</p> <p>This work is sponsored by the Regional and Global Model Analysis (RGMA) program of the Earth and Environmental Systems Sciences Division (EESSD) in the Office of Biological and Environmental Research (BER) within the Department of Energy's (DOE) Office of Science (OS). The work at PCMDI is performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.</p> <p> </p>"},{"location":"99_Acknowlegements/#cmip6plus-cvs","title":"CMIP6Plus CVS","text":"<p>The repository content has been collected from many contributors representing the Coupled Model Intercomparison Project phase 6+ (CMIP6Plus), including those from climate modeling groups and model intercomparison projects (MIPs) worldwide. The structure of content and tools required to maintain it was developed by climate and computer scientists from the Coupled Model Intercomparison Project International Project Office (CMIP-IPO), the Program for Climate Model Diagnosis and Intercomparison (PCMDI) at Lawrence Livermore National Laboratory (LLNL), and the UK MetOffice, with assistance from colleagues at the UK Centre for Environmental Data Analysis (CEDA), the Deutsches Klimarechenzentrum (DKRZ) in Germany and the members of the Infrastructure for the European Network for Earth System Modelling (IS-ENES) consortium.</p> <p>This work is sponsored by the Regional and Global Model Analysis (RGMA) program of the Earth and Environmental Systems Sciences Division (EESSD) in the Office of Biological and Environmental Research (BER) within the Department of Energy's (DOE) Office of Science (OS). The work at PCMDI is performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.</p> <p> </p>"},{"location":"99_Acknowlegements/#wiki-forms-automations-and-infrastructure-work","title":"Wiki, Forms, Automations, and Infrastructure work.","text":"<p>CMIP IPO (Daniel Ellis), LLNL (Paul Durack), MOHC (Matthew Mizielinski)</p>"},{"location":"CMIP6Plus/","title":"What is CMIP6Plus","text":"Description Repository Status <p>This page outlines building guidance to register and contribute to the CMIP6Plus phase. We will update this page with new information as it becomes available.</p>"},{"location":"CMIP6Plus/#current-guidance-for-contributors","title":"Current guidance for contributors:","text":"<ul> <li>Add a new model/source_id</li> </ul>"},{"location":"CMIP6Plus/#additional-resources","title":"Additional resources:","text":"<ul> <li>MIP tables</li> <li>Registered Model Intercomparison Project (MIPs)</li> </ul>"},{"location":"CMIP6Plus/Automations/gencv_action/","title":"[Action] To run create_CV","text":"<p>The flow of the github action which run when a new commits are pushed to a branch. </p> <pre><code>graph TD\n  style AA fill:#003366,stroke:#ffffff,stroke-width:2px;\n  style BB fill:#005cbf,stroke:#ffffff,stroke-width:2px;\n  style CC fill:#0078d4,stroke:#ffffff,stroke-width:2px;\n  style DD fill:#009be1,stroke:#ffffff,stroke-width:2px;\n  style EE fill:#00a8e8,stroke:#ffffff,stroke-width:2px;\n  style FF fill:#00adef,stroke:#ffffff,stroke-width:2px;\n  style GG fill:#33b8ff,stroke:#ffffff,stroke-width:2px;\n  style HH fill:#66c1ff,stroke:#ffffff,stroke-width:2px;\n  style II fill:#99ccff,stroke:#ffffff,stroke-width:2px;\n\n  A[Checkout Repository] --&gt;|1. Checkout| B[Check if Run is Necessary]\n  B --&gt;|2. Determine Necessity| C[Set Up Git]\n  C --&gt;|3. Configure Git Settings| D[Set GIT Repo Environment Variables]\n  D --&gt;|4. Set Environment Variables| E[Display GIT Environment Variables]\n  E --&gt;|5. Display Variables| F[Print Latest Commit SHA]\n  F --&gt;|6. Print SHA and Path| G[Run Python Check]\n  G --&gt;|7. Execute Python Script| H[Write New CV]\n  H --&gt;|8. Write and Commit if Necessary| I[End]\n\n  style A fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style B fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style C fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style D fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style E fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style F fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style G fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style H fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style I fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n\n  A --&gt;|Begin Workflow| B\n  B --&gt;|Determine Necessity based on Git History| C\n  C --&gt;|Configure Git Settings| D\n  D --&gt;|Set Environment Variables| E\n  E --&gt;|Display Variables| F\n  F --&gt;|Print Commit SHA and Path| G\n  G --&gt;|Execute Python Script| H\n  H --&gt;|Write and Commit if Necessary| I\n\n  subgraph AA\n    A\n  end\n\n  subgraph BB\n    B\n  end\n\n  subgraph CC\n    C\n  end\n\n  subgraph DD\n    D\n  end\n\n  subgraph EE\n    E\n  end\n\n  subgraph FF\n    F\n  end\n\n  subgraph GG\n    G\n  end\n\n  subgraph HH\n    H\n  end\n\n  subgraph II\n    I\n  end</code></pre>"},{"location":"CMIP6Plus/Automations/generate_cv/","title":"[Py] Generate CV","text":"<p>The python script used to generate a cv. </p> <p>This is run by the action script [link here]. </p>"},{"location":"CMIP6Plus/Automations/generate_cv/#arguments","title":"Arguments","text":"<pre><code>Create CV \nusage: create_cv.py [-h] [-c COMMIT] [-d DATE] [-t TAG] [-b BRANCH] [-a API]\n\nGithub action script to create CVs\n\noptions:\n  -h, --help            show this help message and exit\n  -c COMMIT, --commit COMMIT\n                        Commit SHA\n  -d DATE, --date DATE  date_commit\n  -t TAG, --tag TAG     tag\n  -b BRANCH, --branch BRANCH\n                        branch\n  -a API, --api API     api_token\n</code></pre>"},{"location":"CMIP6Plus/Automations/generate_cv/#program-flow","title":"Program Flow","text":"<pre><code>\ngraph TB\n\nsubgraph \"Initialize Parameters\"\n    A[Set relative path]\n    B[Set CV prefix]\n    C[Define MIP tables prefix]\n    D[Define table prefix pattern]\nend\n\nsubgraph \"Argument Parsing\"\n    E[Parse commit, date, tag, branch, and API token]\nend\n\nsubgraph \"Define Functions\"\n    F[Read contents from GitHub]\n    G[Read JSON from GitHub]\n    H[Listify function]\n    I[Notnull function]\nend\n\nsubgraph \"Tunable Parameters\"\n    J[Define additional parameters]\nend\n\nsubgraph \"Other Parameters\"\n    K[Define additional parameters]\nend\n\nsubgraph \"Read from MIP Tables\"\n    L[Read source_type, frequency, realm, etc. from MIP tables]\nend\n\nsubgraph \"Main Section\"\n    M[Loop through structure elements]\n    N[Read table_id from GitHub]\n    O[Loop through experiments and update source_type]\n    P[Handle experiment_id entries]\n    Q[Handle activity_id entries]\n    R[Handle source_id entries]\nend\n\nsubgraph \"Metadata and Checksum\"\n    S[Get latest commit from MIP tables]\n    T[Update version metadata]\n    U[Calculate checksum]\nend\n\nsubgraph \"Checksum\"\n    V[Extract branch from branch argument]\n    W[Remove old branched CVs if branch is main]\n    X[Calculate checksum and compare with the old CV]\n    Y[Write updated CV to file]\nend\n\nA --&gt; E\nB --&gt; E\nC --&gt; E\nD --&gt; E\nE --&gt; J\nJ --&gt; L\nE --&gt; F\nE --&gt; G\nJ --&gt; F\nJ --&gt; G\nJ --&gt; H\nJ --&gt; I\nL --&gt; M\nN --&gt; M\nM --&gt; O\nO --&gt; P\nP --&gt; Q\nQ --&gt; R\nR --&gt; T\nT --&gt; U\nU --&gt; V\nV --&gt; W\nW --&gt; X\nX --&gt; Y\n</code></pre>"},{"location":"Data-Analysis-Tools/","title":"FEoCMIP-Data-Analysis-Tools","text":""},{"location":"Data-Analysis-Tools/#this-repository-contains-data-handling-tools-for-cmip-models-contributed-by-members-of-fresh-eyes-on-cmip","title":"This repository contains Data handling tools for CMIP models, contributed by members of Fresh Eyes on CMIP","text":""},{"location":"Data-Analysis-Tools/#this-repository-is-organized-into-the-following-categories","title":"This repository is organized into the following categories:","text":"<ul> <li>Preprocessing</li> <li>Analysis</li> <li>Visualization</li> <li>(ADD MORE HERE)</li> </ul>"},{"location":"Data-Analysis-Tools/#each-python-script-added-should-include-a-env-file-as-well","title":"Each python script added should include a .env file as well","text":""},{"location":"Data-Analysis-Tools/#the-general-layout-of-each-file-is-as-follows","title":"The general layout of each file is as follows:","text":"<ol> <li>Name and contact email of main contributor(s)</li> <li>Purpose of script/function</li> <li>Any other specific information users will need</li> <li>Executable code</li> </ol>"},{"location":"Data-Analysis-Tools/#contributor-information-and-affiliations","title":"Contributor Information and Affiliations","text":"<ul> <li>Keighan Gemmell, University of British Columbia, Canada \ud83c\udde8\ud83c\udde6</li> <li>J\u00falia Crespin Esteve, Universitat de Barcelona, Spain</li> <li>Anja Katzenberger, Potsdam Institute of Climate Impact Research, Germany </li> <li>ADD YOUR NAME AND AFFILIATION HERE </li> </ul>"},{"location":"Data-Analysis-Tools/Preprocessing/preprocess_parallel/","title":"preprocess parallel","text":"<p>This is xxxx</p> <p>```python </p>"},{"location":"Data-Analysis-Tools/Preprocessing/preprocess_parallel/#created-by-keighan-gemmell-keighanchemubcca","title":"Created by Keighan Gemmell (keighan@chem.ubc.ca)","text":""},{"location":"Data-Analysis-Tools/Preprocessing/preprocess_parallel/#this-file-contains-a-wrapped-preprocessing-function-that-detrends-removes-seasonal-cycle-and-executes-a-rolling-average","title":"This file contains a wrapped preprocessing function that detrends, removes seasonal cycle, and executes a rolling average","text":""},{"location":"Data-Analysis-Tools/Preprocessing/preprocess_parallel/#there-is-also-a-normalization-function","title":"There is also a normalization function","text":""},{"location":"Data-Analysis-Tools/Preprocessing/preprocess_parallel/#to-run-this-script-you-will-need-ensure-xarray-is-installed","title":"To run this script you will need ensure xarray is installed","text":""},{"location":"Data-Analysis-Tools/Preprocessing/preprocess_parallel/#change-file-paths-to-where-your-data-is-stored","title":"Change file paths to where your data is stored","text":"<p>import xarray as xr import numpy as np from multiprocessing import Pool import s3fs import geocat.comp import time import pandas as pd import json import os import sys import pyximport pyximport.install(setup_args={\"include_dirs\":np.get_include()},                   reload_support=True) sys.path.insert(0,'') #fill in path here</p>"},{"location":"Data-Analysis-Tools/Preprocessing/preprocess_parallel/#set-of-functions-for-executing-preprocessing-of-cmip6-data","title":"Set of functions for executing preprocessing of CMIP6 data","text":""},{"location":"Data-Analysis-Tools/Preprocessing/preprocess_parallel/#this-code-used-an-old-aws-s3-storage-system-to-access-cmip6-data-you-will-need-to-add-the-lines-to-pull-data-from-wherever-you-store-it","title":"This code used an old AWS S3 storage system to access CMIP6 data, you will need to add the lines to pull data from wherever you store it","text":"<p>df = pd.read_csv(\"\") #name of file here</p> <p>proc = 40</p> <p>def getData(query:dict):     '''     Load CMIP6 data into xarray dataframe     query (dict or str) - dict or str with data information                         - if dict format as {'param':'value','param2':['val1','val2']}     '''     # Create query string for pandas.DataFrame.query     if type(query) is dict:         inputStr = \" &amp; \".join([\"{0}=='{1}'\".format(param, query[param]) for param in query])     elif type(query) is str: # if its already a string, pass through         inputStr=query</p> <pre><code># Searches cmip6 data csv for datasets that match given parameters\ndf_subset = df.query(inputStr)\nif df_subset.empty:\n    print('data not available for '+inputStr)\nelse:\n    # load data\n    for v in df_subset.zstore.values:\n        zstore = v\n        mapper = fs.get_mapper(zstore)\n        ### !!!! Note decode times is false so we can use integer time values !!!!\n        ### open_zarr, so datasets are not loaded yet\n        return_ds = xr.open_zarr(mapper, consolidated=True,decode_times=False)\nreturn(return_ds)\n</code></pre> <p>def removeSC(x:np.ndarray):     '''     Removes seasonal cycle from monthly data     x (np.ndarray) - 3D (time,lat,lon) numpy array     '''     nt,nx,ny = x.shape # get array dimensions     nyears = nt//12     # get month means     monmean = np.mean(x.reshape(nyears,12,nx,ny),axis=0)     for m in range(12): #for each month         x[m::12] = x[m::12] - monmean[m]     return x</p> <p>def detrend(x:np.ndarray,time:np.ndarray):     '''     remove degree three polynomial fit     x (np.ndarray) : 3D (time, lat, lon) numpy array     time (np.ndarray) : 1D (time) array      '''     nt,nx,ny = x.shape     xtemp = x.reshape(nt,nxny)     p = np.polyfit(time, xtemp, deg=3)     print(p.shape)     fit = p[0](time[:,np.newaxis] 3)+ p[1]*(time[:,np.newaxis]2) + p[2]*(time[:,np.newaxis]) + p[3]     return x - fit.reshape(nt,nx,ny)</p> <p>def preprocess(data:np.ndarray, tdata:np.ndarray, window:int = 31,proc:int = 40):     '''     Executes the three steps of the preprocessing (deseasonalize, detrend, normalize)     data (np.ndarray) : data to process     t (np.ndarray) : time values     window (integer) : years for the rolling average     proc (int) : number of processes for multiprocessing     '''</p> <pre><code>### remove seasonal cycle\nt1 = time.time()\nprint('Deseasonalize')\ndeseas = removeSC(data)\nt2 = time.time()\nprint('Time : ', t2-t1)\n\n### remove cubic fit\nprint('Detrend')\nprint(tdata.shape)\ndetre = detrend(deseas,tdata)\nt3 = time.time()\nprint('Time : ',t3-t2)\n\nx = detre\n\n## function for normalizing - written for passing to multiprocessing\nglobal normalize\ndef normalize(t):\n    '''\n    Calculate anomaly and normalize (repeat boundary values) for monthly data\n    averages across years (12 time step skips)\n    x (np.ndarray) - integer values\n    t (integer) - time\n    window (integer) - years in the averaging window must be odd for now\n    '''\n    assert window%2 == 1\n    tmax = x.shape[0]\n    halfwindow = window//2\n    yr = t//12\n    mon = t%12\n    selx = np.zeros_like(x[:window])\n    # get rolling window (backfills/forward fills with first/last value)\n    if t-halfwindow*12 &lt; 0:\n        selx[:halfwindow - yr] = x[mon]\n        selx[halfwindow-yr:] = x[mon:t+halfwindow*12+1:12]\n    elif t+halfwindow*12+1 &gt; tmax:\n        selx[halfwindow + (tmax//12 - yr):] = x[tmax- 12 + mon]\n        selx[:halfwindow + (tmax//12 - yr)] = x[t - halfwindow*12::12]\n    else:\n        selx = x[t-halfwindow*12:t+halfwindow*12+1:12]\n    # calculate normalized\n    normed = (x[t] - np.mean(selx,axis=0))/np.std(selx,axis=0)\n    return normed\n### run normalization\nprint('Normalize')\nntime = x.shape[0]\n\n# parallelize normalizing across time (trivially parallel)\nwith Pool(processes=proc) as pool: \n    outdata = pool.map(normalize, range(ntime))\nt4 = time.time()\nprint('Time : ',t4-t3)\nreturn np.array(outdata,dtype=np.float32)\n</code></pre> <p>def run_preprocess(experiment:str, modelName:str, member:str, variables_in:list,             variables_out:list, attribute_path:str = 'variable_defs.json',             out_path:str = None,lf_query:str = None,             append:bool=False,frequency:str='Amon'):     '''     Wrapper function that executes the preprocessing pipeline     experiment (str) : experiment name (experiment_id)     modelName (str) : model name (source_id)     member (str) : ensemble member code (member_id) ripf     variables_in (list) : list of variables to grab from CMIP6 archive     variables_out (list) : list of variables to calculate and output     attribute_path (str) : path to a json file with variable descriptions.         Note if you request a variable in variables_out that is not a CMOR variable, it must         be defined in the attribute .json     out_path (str) : file name for output     lf_query (str) : query string to pass to pandas.DataFram.query  recommended to pass this,         otherwise the script is likely to fail due to not finding sftlf variable         set to False to skip adding the landfraction, set to None to attempt to find land fraction         for the selected experiment and ensemble member     append (str) : append variables_out to an existing netcdf file     frequency (str) : CMIP table to get data from (e.g. Amon, Omon, Aday - table_id).      '''     # Default out_path     if out_path is None:          out_path = ''.format(modelName, member, experiment)</p> <pre><code>print('Preprocessing for {0} {1} {2}'.format(modelName,experiment,member))\n\n# if we are appending to an existing netcdf\n# load the dataset and determine the existing variables\nif append:\n    if os.path.isfile(out_path):\n        existing = xr.open_dataset(out_path)\n        existing_variables = list(existing.variables)\n        del existing\n    else:\n        print('append = True, but no existing file. Setting to append = False')\n        append = False\n\n## Load in data (could make faster with open_mfdataset?)\ndict_query = {'source_id':modelName, 'table_id':frequency, 'experiment_id':experiment, 'member_id':member}\ndata = {}\nfor var in variables_in:\n    dict_query['variable_id'] = var\n    data[var] = getData(dict_query)\n\n# Load land fraction data\nif lf_query is None:\n    # if query isn't provided, attempt to load from current experiment\n    lf_query  = \"source_id=='{0}' &amp; table_id=='fx' &amp; experiment_id=='{1}' &amp;  member_id=='{2}' &amp; variable_id=='sftlf'\".format(modelName,experiment,member)\nelif lf_query: # set lf_query=False to skip\n    data['sftlf'] = getData(lf_query) ## land area fraction as %\n\n# Attributes for derived variables\nd_attr = json.load(open(attribute_path))\n\nprocessed = {} # preprocessed data\norig = {} # raw data\nfor var in variables_out:\n    if append and var in existing_variables:\n        # skip existing variables if in append mode\n        print('{0} already exists, skipping'.format(var))\n        variables_out.remove(var)\n        continue\n    print('-----------------')\n    print(var)\n    print('-----------------')\n    if var in variables_in: \n        # If taking variable directly from CMIP\n        signs = [1]\n        variables = [var]\n        attributes = {'long_name': data[var][var].long_name, 'units':data[var][var].units}\n    elif var in d_attr: \n        # If calculating a variable from CMIP variables\n        signs = d_attr[var]['signs']\n        attributes = d_attr[var]\n        variables = d_attr[var]['variables']\n    else: # Something's wrong (missing variable or definition)\n        print('missing variable definition in {0}'.format(attribute_path))\n        continue\n    if 'integral' in attributes:\n        # if we are integrating a 3D variable\n        processed[var], orig[var] = calc_var_integral(data,variables[0],var,attributes = attributes)\n    else:\n        processed[var], orig[var] = calc_var(data, variables, signs, var,attributes = attributes)\n\n# Cast the fixed land fraction data into a time series\nif lf_query: # set lf_query=False to skip\n    # tile in time\n    sftlfOut, b2 = xr.broadcast(data['sftlf'].sftlf, data[variables_in[0]][variables_in[0]], exclude=('lat','lon'))\n    sftlfOut=sftlfOut.where(sftlfOut==0,1) ### data has values as 0 for ocean or 100 for land sa making 100 as 1\n    lsMask = xr.DataArray(name='lsMask', data=sftlfOut.load(), attrs={'long_name':'land_sea_mask', 'Description':'land fraction of each grid cell 0 for ocean and 1 for land'}, \n                        coords={'time': data[variables_in[0]].time,'lat': data['sftlf'].lat,'lon': data['sftlf'].lon})\n\n# change time to original values (time values get altered in calc_var)\nfor var in variables_out:\n    processed[var]['time'] = data[variables_in[0]].time\n    orig[var]['time'] = data[variables_in[0]].time\n\n# Save one DataArray as dataset\nvar = variables_out[0]\noutput_ds = processed[var].to_dataset(name = var+'_pre')\noutput_ds[var] = orig[var]\nfor var in variables_out[1:]:\n    # Add next DataArray to existing dataset (ds)\n    output_ds[var+'_pre'] = processed[var]\n    output_ds[var] = orig[var]\n# add the landfraction query\nif lf_query:\n    output_ds['lsMask'] = lsMask\nprint('Write to file')\nif append:\n    output_ds.to_netcdf(path=out_path,mode='a',format='NETCDF4')\nelse:\n    output_ds.to_netcdf(path=out_path,mode='w',format='NETCDF4')\n    ```\n</code></pre>"},{"location":"Data-Analysis-Tools/Visualisation/change_in_windvector/","title":"Change in Wind speed and Direction","text":"<pre><code>#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated by anjakatzenberger (anja.katzenberger@pik-potsdam.de)\n\"\"\"\n\n### This code provides changes in wind speed and wind direction for a subset of CMIP6 models\n### The first figure (a panel of 2x3 wind fields) is written for 6 CMIP6 models\n### The second figure gives a multi model mean of the models given\n### The input data is preprocessed using CDOs (selected time period, selected altitude of winds, as well as mean over time for specific months)\n### Adapt the preprocessing as adequate for your research question\n</code></pre>"},{"location":"MIP_Tables/","title":"mip-cmor-tables","text":"<p>JSON tables to create Model Intercomparison Project (MIP) datasets</p> <p>A quick intro on how to make use of these tables is available on the Wiki.</p>"},{"location":"MIP_Tables/#contributors","title":"Contributors","text":"<p>Thanks to our contributors!</p>"},{"location":"mailing_lists/whitelisting/","title":"Whitelisting the CMIP server","text":"<p>As our mail server is relatively new, although all the required message headers and metadata have been set, emails may occasionally end up in the spam/quarantine folder. </p> <p>If you are having trouble recieving emails, or these are being flagged, you may have to add the WCRP-cmip domain to your whitelist. The most common solutions are provided below, although should you have any others, feel free to contact us, and we will add them to the list. </p>"},{"location":"mailing_lists/whitelisting/#microsoft","title":"Microsoft","text":""},{"location":"mailing_lists/whitelisting/#1-open-the-safe-senders-list","title":"1. Open the safe-senders list","text":"<p>https://outlook.office365.com/mail/options/mail/junkEmail/safeSendersDomainsV2</p>"},{"location":"mailing_lists/whitelisting/#2-add-the-wcrp-cmip-domain","title":"2. Add the WCRP-CMIP domain","text":"<p>We now need to add our domain to be whitelisted. The domains are: <pre><code>wcrp-cmip.org\nmail.wcrp-cmip.org\n</code></pre></p> <p></p>"}]}